#ifdef __CCE_KT_TEST__
#define __aicore__
#else
#define __aicore__ [aicore]
#endif

extern "C" __global__ __aicore__ void preload_stack_16KB_mix_aic()
{
    dcci((__gm__ uint32_t *)0, 1);
    asm("MOV X2, CTRL" ::: "X2");  // Backup the Control register

    // Set 49th bit of CTRL to 1
    // i.e, set the SU UB access cachable
    // Setting the bit directly on the control register does not work
    asm("MOV X3, CTRL" ::: "X3");
    asm("MOVI X4, #49" ::: "X4");
    asm("SBITSET1.b64 X3, X4" ::: "X3");
    asm("MOV CTRL, X3" ::: "CTRL");

    // Calculate load address to be = SYS_VA_BASE + (COREID +1) * 32K + 0x10 0000
    // load top 16K of stack from bottom to up
    asm("MOV X16, SYS_VA_BASE" ::: "X16");
    asm("MOVI X18, #0" ::: "X18");
    asm("MOVK X18, #16, #1" ::: "X18");  // generate 0x00 10 0000
    asm("MOVI X4, #32768" ::: "X4");     // X4 = 32K
    asm("MOVI X10, #0x7FFF" ::: "X10");  // X10 = 32K -1; used for masking
    asm("MOVI X9, #16384" ::: "X9");     // X9 = 16K
    asm("MOV X15, COREID" ::: "X15");
    asm("AND.b64 X15, X10, X15" ::: "X15");  // Mask incorrect bits in the Core ID
    asm("MUL.s64 X5, X4, X15" ::: "X5");     // X5 = COREID * 32K
    asm("ADD.s64 X6, X5, X18" ::: "X6");     // X6 = COREID * 32K + 0x10 0000
    asm("ADD.s64 X6, X6, X9" ::: "X6");      // X6 = COREID * 32K + 16K + 0x10 0000
    asm("ADD.s64 X6, X6, X16" ::: "X6");     // X6 = COREID * 32K + 16K + 0x10 0000 + SYS_VA_BASE
    // Load the data from the address
    // Load the stack from the bottom to the top, each load is 64b
    asm("MOVI X7, #0");
    asm("MOVI X8, #6");
    asm("MOVI X9, #128");
    asm("JUMPCMP.GE.s64 X8, X7, X9");
    asm("LD_IO.b64 X0, [X6], #0, #0" ::: "X0");  // 2 loads per loop
    asm("LD_IO.b64 X0, [X6], #64, #0" ::: "X0");
    asm("ADDI X7, X7, #1");
    asm("ADDI X6, X6, #0x80");
    asm("JUMPI #-5");
    asm("MOV CTRL, X2" ::: "CTRL");
    asm("BAR.ALL");
    // trap();
}